# 经验积累

<!--记录一些平时开发中需要注意的点-->

- 如果某个接口有用到缓存，最好在接口请求的时候加一个是否走缓存的字段，这个字段可以不用对外暴露，它的作用是如果线上改了数据库等需要更新缓存，或者改了逻辑需要验证的时候就可以加上这个字段请求一下。不用等到缓存到期，有些缓存的时间比较长非常不好验证。

- 后端开发一个对外提供的http接口的时候，我们应该站在一个调用者的角度去思考接口的返回是否方便调用者调用，举个例子，查询user 的列表数据，返回的字段有id和userId这两个字段，查询user详情的接口的入参到底用这两个id哪个id呢？如果是用id来区分唯一的一条记录的，那么这个userid这个字段时候还需要是不是名字有问题，或者改为userCode.如果不用数据库唯一主键id来做区分，用自己的userId,那么数据库唯一主键id在接口中就不要返回，以免引起调用者的误解。tip 虽然是个小小的问题，但是如果设计好了（最好是用数据库唯一id来区分）无论从接口的开发，文档的定义，数据库的设计，多开发人员协作开发上都有很大的好处。

- 关于微服务拆分之后的为满足需求的各种查询的思考，举个例子：有一个用户服务 user-service,有一个技能服务 skill-service。需求：查询某个用户下的技能（通过技能的一些属性进行过滤）。skill-service对应的数据库里面有 skill（技能表） user_skill（用户技能关系表），那么查询的时候没什么问题直接连接查询就可以了。但是也有这种情况，skill表和user_skill不在一个表里面，比如skill存在mongodb user_skill却存在mysql里，那这种情况的查询在设计的时候就应该考虑把skill条件查询的字段通过接口调用 mq等方式冗余到user_skill这个库里面。

- 创建一个maven的java项目的时候用什么结构呢，是父子项目（dao、service、controller是单独的子项目）？还是就一个单个的pom项目（dao、service、controller在一个项目中）。虽然都可以，但是父子项目相比开发比较麻烦，有什么不同人员开发开发有不同的理解，也有可能分子项目分的太细，把entity，Model这些都分开，这样及其不好。根据我们的项目的部署以及功能需要可以这样创建项目。

  1、如果项目是提供微服务的这种方式，独立部署的，创建一个项目 分为service和api这两个子项目，api就是声明接口的，service是逻辑，entity vo等放在api层 service应用api层

  2、如果项目是web和app服务单独部署的有给web提供的controller  也有给app提供的gateway 那么项目应该分开发service 、controller、dao也可以分开。entity vo 单独分一个common 只能其他层引用common
  
- 如果一个需求要评审，那么就应该通知相关开发人员，因为在需求评审的时候，其实就是在开发人员脑海中构思的时候，什么地方有问题，合不合理他就是最有发言权的。评审过后没问题着手开发的时候，我们应该先考虑怎么测试，或者说怎么自测。如果是个简单的接口那很简单，但是如果不是，假如说是类似上报数据，我们拿到数据做计算，做统计，这种的我们就应该先根据上报的协议写一个模拟上报的程序。虽然复杂但是为以后的开发打好了基础。也可以给团队人员共享。

- 关于微服务多库基础数据同步的问题：
  问题：
  1、服务划分为微服务之后一个微服务对应一个数据库
  2、那么导致基础数据维护在不同的数据库,
  3、在很多的查询当中有一些连接查询需要用到这些基础数据
  4、先基础数据和别的数据都通过服务调用的方式查询出来然后循环组合（这个方式比较复杂，而且有些数据是查询条件的话就不好实现）
  5、通过把需要的基础数据在不同的库里面也维护一张表，然后通过连接查询的方式查询。这样就存在数据同步的问题，如果基础数据修改了怎么能保证数据的一致性
  6、如果基础服务修改了基础数据通过MQ的方式发送给维护了基础数据的服务，修改，这种方式不需要MQ的强一致性，我们会通过每天晚上定时任务同步修改了的数据。
  7、所以每个表的创建修改时间的维护非常重要。
  
- 权限相关的数据库id不建议用自增长的id。

- 如果查询的List是null，那么返回一个空的集合。return Lists.newArrayList();

- 在向喜马拉雅回传数据的需求中有这样一个接口，接口回传请求喜马拉雅的次数和请求其他资源的次数，数据是大数据通过埋点的方式通过kafka发送到服务端，服务端通过消费kafka的消息获得数据，组装数据通过喜马拉雅提供的接口返回给喜马拉雅。通过喜马拉雅的接口文档可以看出，这个数据可以一批一批的传。在服务端收集到数据的时候怎么向喜马拉雅传输呢？有一下的方案：

  - 接受好一天的数据然后再通过调用接口给喜马拉雅，（数据有延迟）
  
  - 从kafka收到数据就回传，相当于一条一条的传输  （喜马拉雅的接口是否有限流，在流量高峰依赖第三方的忌口的稳定性是不可控的，一旦把接口给弄挂了，服务端的消息来不及消费，导致kafka消息堆积等影响依赖kafka的其他业务等等）
  
  - 一部分一部分的发送，比如消费10条kafka的消息发送一个http请求，但是这样也有个问题，10条有可能1秒内就有10条 也有可能好多分钟了还不够10条，所以有可能出现数据延迟。怎么办呢，我们可以像kafka一样处理这种情况，我们知道kafka有这么两个参数，batch.size和linger.ms 。batch.size是没个批次的大小，linger.ms是如果多少毫秒没有够这个大小就发送，防止延迟。
  
    怎么去实现这种机制呢。我们把消息放在一个固定大小的同步的集合里面。定义一个延时的时间，一个线程监控如果集合满了就立即发送，并且延时时间清0，一个线程监控延时的时间如果时间到了立即发送，并且延时时间清0。
  
- 关于设计模式其实是一种思想，对于设计模式的理解也不要仅仅局限在代码上，设计模式是解决方案，重要的是编程思想，而不是固定的模板。比如两个系统之间需要调用，但是由于种种原因不能直接相互调用，这时候我们就可以用适配器模式，加一个中间的适配的系统。

- 在项目开发的时候，应该要分清主次，什么是主要业务，什么是次要业务，所有的业务都不能影响到主要服务的运行。比如说，一个接口的查询，要在这个接口上加一个接口调用的统计需要埋点，那这时候接口就是主要业务，埋点不能影响查询，埋点可以通过线程池异步的方式进行，并且线程池队列满了之后的策略最好是丢弃，或者是抛异常，然后我们通过记录日志或者邮件告警知道异常再进行处理，不能用主线程去执行。这样会影响到接口的查询。

- 从需求的评审就开始在脑海中构思业务逻辑以及代码实现，比如说状态切换的操作、审核的操作等这些按钮放在编辑里面对业务的实现就是不友好额，因为逻辑混淆，最好是放在列表做成单独的操作，这样的好处逻辑清楚，功能分开，条理清楚。不容易出bug，测试在测试的时候也能更好测试。

- 合理的选择开源组件，并且合理的选择开源组件的功能，比如说我们需要把项目的配置文件集中管理，选择携程开源的apollo。那么我们要考虑我们只需要他的配置文件功能，
  配置文件功能里面又有 namespace cluster等概念。我们是否需要。根据我们自己当前的实际情况合理选择。多余的选择只能增加项目的复杂度。

